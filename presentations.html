<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Mariela Rajngewerc</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

								<header id="header">
								<a href="index.html" class="logo"><strong>Mariela  Rajngewerc</strong></a>
									<ul class="icons">
										<li><a href="https://twitter.com/mariela_rajng" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
										<li><a href="https://github.com/marielaraj" class="icon brands fa-github"><span class="label">Github</span></a></li>
										<!--<li><a href="https://github.com/marielaraj" class="fa fa-envelope" aria-hidden="true"><span class="label"></span></a></li> -->
										
									</ul>
								</header>

							<!-- Content -->
								<section>
									<header class="main">
										<h1>Presentations</h1>
									</header>


								</section>
									<section>
								      <h3>2023</h3>

								    <ul>
  									<li>
								      	<article>
										  <div>
								    <div>
								        <h4>Analysis of fairness metrics for anonymization in Electronic Health Records</a></h4>
								        
								      
								      <h5>Authors: Mariela Rajngewerc, Laura Acion and Laura Alonso Alemany</h5>
								      <p style="color:black">Presented at KHIPU 2023 - Montevideo, Uruguay. March 2023.</p>

									<p style="color:black"> <b> Abstract: </b>Classical metrics to evaluate machine learning models are usually aggregates that provide no insights into the differential behavior of the model with respect to certain subgroups, which is usually known as bias. When working with models that will affect human beings, the impact of bias must be assessed to detect, mitigate or even prevent possible harm.

									Several fairness metrics have been defined in the bibliography. In some cases, if a metric adequately represents a relevant aspect of the behavior of the model, this implies that some other metrics may be irrelevant. Different problems may require different perspectives and different bias definitions.

									In this work, we show the strengths and limitations of different metrics, illustrating them as applied to the bias analysis of anonymization algorithms of Electronic Health Reports (EHR). These algorithms take a set of sentences and eliminate any sensitive data they may contain (names, surnames, identification numbers, etc). If these algorithms make systematic errors over a specific group of society, that group may be exposed, and their privacy may be violated. We show how different fairness metrics highlight certain aspects of the behavior of these algorithms while obscuring others. </p>
								    


									 <div class="columns download">
							          <p>
							             <a href="/home/okus/Desktop/gitprojects/marielaraj.github.io/files/mrajngewerc_poster_khipu_2023.pdf" class="button" download>Poster KHIPU 2023</a>
							          </p>
							     </div>

								    </div>
								  </div>
									</li>
									</ul>  

																	  
									  
									</article>

									      
									    </section>

						</div>
					</div>

				<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">

							<!-- Search -->
								<section id="search" class="alt">
									<form method="post" action="#">
										<input type="text" name="query" id="query" placeholder="Search" />
									</form>
								</section>

							<!-- Menu -->
							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>Menu</h2>
									</header>
									<ul>
										<li><a href="index.html">Homepage</a></li>
										<!-- <li><a href="generic.html">Publicaciones</a></li> -->
										<li><a href="presentations.html">Presentations</a></li>
										<!-- <li><a href="generic.html">Docencia</a></li> -->
										<!-- <li><a href="elements.html">CV</a></li> -->
										<li><a href="#contact" id="contact-link">Contact</a></li>
										<!-- <li>
											<span class="opener">Submenu</span>
											<ul>
												<li><a href="#">Lorem Dolor</a></li>
												<li><a href="#">Ipsum Adipiscing</a></li>
												<li><a href="#">Tempus Magna</a></li>
												<li><a href="#">Feugiat Veroeros</a></li>
											</ul>
										</li>
										<li><a href="#">Etiam Dolore</a></li>
										<li><a href="#">Adipiscing</a></li>
										<li>
											<span class="opener">Another Submenu</span>
											<ul>
												<li><a href="#">Lorem Dolor</a></li>
												<li><a href="#">Ipsum Adipiscing</a></li>
												<li><a href="#">Tempus Magna</a></li>
												<li><a href="#">Feugiat Veroeros</a></li>
											</ul>
										</li>
										<li><a href="#">Maximus Erat</a></li>
										<li><a href="#">Sapien Mauris</a></li>
										<li><a href="#">Amet Lacinia</a></li> --> 
									</ul>
								</nav>

							<!-- Section -->

							<!-- Section -->
							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">&copy; Untitled. All rights reserved. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
								</footer>

						</div>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>